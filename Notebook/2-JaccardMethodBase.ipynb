{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "2-JaccardMethodBase.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "XamxwSMMX5D5"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  import tensorflow.compat.v2 as tf\n",
        "except Exception:\n",
        "  pass\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from tqdm.auto import tqdm\n",
        "import csv\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZBAv1UrX5D8"
      },
      "source": [
        "# Reload Data and Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e-ogVxQas3n"
      },
      "source": [
        "# Download caption annotation files\n",
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                            cache_subdir=os.path.abspath('.'),\n",
        "                                            origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                            extract = True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "# Download image files\n",
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "D2rrXKIgX5D9"
      },
      "source": [
        "annotation_file = './annotations/captions_train2014.json'\n",
        "\n",
        "PATH = './train2014'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Syx5QB0sX5D9"
      },
      "source": [
        "# Read the json file\n",
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Store captions and image names in vectors\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    full_coco_image_path = os.path.join(PATH, 'COCO_train2014_' + '%012d.jpg' % (image_id))\n",
        "\n",
        "    all_img_name_vector.append(full_coco_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "# Shuffle captions and image_names together\n",
        "# Set a random state\n",
        "train_captions, img_name_vector = shuffle(all_captions,\n",
        "                                          all_img_name_vector,\n",
        "                                          random_state=1)\n",
        "\n",
        "# Select the first 30000 captions from the shuffled set\n",
        "num_examples = 30000\n",
        "train_captions = train_captions[:num_examples]\n",
        "img_name_vector = img_name_vector[:num_examples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "t87tlhDlX5D-"
      },
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Fct96bfUX5D-"
      },
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "oyCjDzoxX5D_"
      },
      "source": [
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "for img, path in tqdm(image_dataset):\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "kAda2zQbX5D_"
      },
      "source": [
        "# Find the maximum length of any caption in our dataset\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "\n",
        "# Choose the top 5000 words from the vocabulary\n",
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "#train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "\n",
        "# Pad each vector to the max_length of the captions\n",
        "# If you do not provide a max_length value, pad_sequences calculates it automatically\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
        "\n",
        "# Calculates the max_length, which is used to store the attention weights\n",
        "max_length = calc_max_length(train_seqs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "nGST7lcuX5EA"
      },
      "source": [
        "# Create training and validation sets using an 80-20 split\n",
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    cap_vector,\n",
        "                                                                    test_size=0.0333,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSo8VZWdX5EA"
      },
      "source": [
        "# NEW: Work with Caption to Got Most Similar Images with Your Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2HBnDlFX5EA"
      },
      "source": [
        "## Reload previous Results\n",
        "Here you can start to test your model. We provide you a baseline model that use BLEU score in order to compare 2 captions.\n",
        "This is a first approach and you have to improve it! \n",
        "\n",
        "Loading the Data and \"all_captions.csv\" file you don't have to train again all image captioning model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "vPkMUdN6X5EB"
      },
      "source": [
        "all_captions = pd.read_csv(\"all_captionsNEW.csv\", sep=',') \n",
        "\n",
        "real_captions = [x.split() for x in all_captions['true_caption'].tolist()]\n",
        "pred_captions = [x.split() for x in all_captions['pred_caption'].tolist()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "-Ugce_5ZX5EB"
      },
      "source": [
        "len(pred_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hboCEvFLX5EB"
      },
      "source": [
        "## Use Jaccard as Similarity Criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "FPU-0cK2X5EB"
      },
      "source": [
        "import warnings\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlAX3WdkZr13"
      },
      "source": [
        "def jaccard_similarity(query, document):\n",
        "    intersection = set(query).intersection(set(document))\n",
        "    union = set(query).union(set(document))\n",
        "    return len(intersection)/len(union)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAYJTGpTZsmO"
      },
      "source": [
        "# Jaccard Example\n",
        "a = ['a', 'dog','over','a','bike']\n",
        "b = ['a', 'cat','over','a','bike']\n",
        "jaccard_similarity(a,b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "fEMOihUOX5EC"
      },
      "source": [
        "def get_similar_result_jaccard(idx, real_captions, pred_captions):\n",
        "\n",
        "    s_score_list = []\n",
        "\n",
        "    for idx_2 in range(len(pred_captions)):\n",
        "      \n",
        "      s_score = jaccard_similarity(real_captions[idx], pred_captions[idx_2])\n",
        "      s_score_list.append((idx_2, s_score))\n",
        "\n",
        "    s_score_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return s_score_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZPAqxQFX5EC"
      },
      "source": [
        "## Create File with Your Submission Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "dl-9_Wl2X5EC"
      },
      "source": [
        "def create_submission_file(top_k, img_name_val, real_captions, pred_captions):\n",
        "\n",
        "    with open('./submission_jaccard.csv', 'w') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"caption\", \"image_list\"])\n",
        "\n",
        "        for idx in tqdm(range(len(img_name_val))):\n",
        "\n",
        "            s_score_res = get_similar_result_jaccard(idx, real_captions, pred_captions)\n",
        "\n",
        "            writer.writerow([' '.join(real_captions[idx]), ' '.join(list(map(lambda x: str(x[0]), s_score_res[:top_k])))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "ySGQ3orEX5EC"
      },
      "source": [
        "create_submission_file(len(img_name_val), img_name_val, real_captions, pred_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_b2nfm7X5EC"
      },
      "source": [
        "## View Some Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gIqLYeFX5ED"
      },
      "source": [
        "### Show Qualitative Results for a Choosen Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORFO0THXl1mf"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "3NQIzuIiX5ED"
      },
      "source": [
        "def show_image(image_fname, new_figure=True):\n",
        "  if new_figure:\n",
        "    plt.figure()\n",
        "  np_img = cv2.imread(image_fname)\n",
        "  np_img = cv2.cvtColor(np_img, cv2.COLOR_BGR2RGB)\n",
        "  plt.imshow(np_img) \n",
        "\n",
        "def show_qualitative_results(idx1, top_k=20):\n",
        "\n",
        "    b_score_res = get_similar_result_jaccard(idx1, real_captions, pred_captions)\n",
        "\n",
        "    print(\"Real capt:\", ' '.join(real_captions[idx1]))\n",
        "    print(\"Pred capt:\", ' '.join(pred_captions[idx1]))\n",
        "    sentence1 = [w for w in real_captions[idx1] if not w in stop_words]\n",
        "    sentence2 = [w for w in pred_captions[idx1] if not w in stop_words]\n",
        "    ss = jaccard_similarity(sentence1, sentence2)\n",
        "    print(\"Score with True Predicted caption:\", ss)\n",
        "    print()\n",
        "\n",
        "    show_image(img_name_val[idx1], new_figure=False)\n",
        "    plt.grid(False)\n",
        "    plt.ioff()\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "\n",
        "    for idx2, (idx, sim_val) in enumerate(b_score_res[:20]):\n",
        "        print(idx, sim_val, ' '.join(pred_captions[idx]))\n",
        "        plt.subplot(4, 5, idx2+1)\n",
        "        show_image(img_name_val[idx], new_figure=False)\n",
        "        plt.grid(False)\n",
        "        plt.ioff()\n",
        "        plt.axis('off')\n",
        "        plt.title('{}'.format(idx2+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "hxdtbO0aX5ED"
      },
      "source": [
        "show_qualitative_results(idx1 = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ9y4qJVX5ED"
      },
      "source": [
        "### Show Distribution of Right Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Z3U7NlJeX5ED"
      },
      "source": [
        "all_idx = []\n",
        "top_k = 1000\n",
        "\n",
        "for ref_idx in tqdm(range(len(img_name_val))):\n",
        "    s_score_res = get_similar_result_jaccard(ref_idx, real_captions, pred_captions)\n",
        "    list_res = list(map(lambda x: x[0], s_score_res[:top_k]))\n",
        "    index = list_res.index(ref_idx)\n",
        "    all_idx.append(index)\n",
        "\n",
        "n, bins, patches = plt.hist(all_idx, bins=1000)\n",
        "plt.xlabel('top K')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}